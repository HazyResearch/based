{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed-decay with projections\n",
    "\n",
    "The objective of this notebook is to provide an example of the fixed-decay approach discussed in the Based paper (https://arxiv.org/abs/2402.18668). While Based achieves high-quality with *no* decay whatsoever, the following addition may be helpful to your use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "\n",
    "b, h, n, d, f = 2, 4, 64, 16, 16\n",
    "eps = 1e-12\n",
    "d_model = h * d\n",
    "q = torch.randn(b, h, n, f)\n",
    "k = torch.randn(b, h, n, f)\n",
    "v = torch.randn(b, h, n, d)\n",
    "hidden_states = torch.randn(b, n, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the fixed decay matrices\n",
    "\n",
    "class DecayClass(nn.Module):\n",
    "    def __init__(self, l_max, decay_const=-3, decay_denom=False, n_kv_heads=16):\n",
    "        super().__init__()\n",
    "        self.l_max = l_max\n",
    "        assert self.l_max > 0, print(f'double check l_max')\n",
    "        decay_const = decay_const\n",
    "        self.decay_denom = decay_denom\n",
    "        self.num_heads = n_kv_heads\n",
    "        decay = torch.log(1 - 2 ** (decay_const - torch.arange(self.num_heads, dtype=torch.float)))\n",
    "        self.register_buffer(\"decay\", decay)\n",
    "    \n",
    "    def forward(self):\n",
    "        index = torch.arange(self.l_max).to(self.decay)\n",
    "        mask = torch.tril(torch.ones(self.l_max, self.l_max).to(self.decay))\n",
    "        mask = torch.masked_fill(index[:, None] - index[None, :], ~mask.bool(), float(\"inf\"))\n",
    "        mask = torch.exp(mask * self.decay[:, None, None])\n",
    "        mask = torch.nan_to_num(mask)\n",
    "        if self.decay_denom:\n",
    "            mask = mask / mask.sum(dim=-1, keepdim=True).sqrt()\n",
    "        return mask, torch.exp(self.decay)\n",
    "\n",
    "\n",
    "decay_cls = DecayClass(l_max=n, decay_const=-3, decay_denom=False, n_kv_heads=h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/cr05_data/sim_data/miniconda3/envs/based/lib/python3.8/site-packages/tqdm-4.66.1-py3.8.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# plug into linear attention\n",
    "\n",
    "# Version 1: default, no decay (https://github.com/HazyResearch/based/blob/9db60a33d20e6c024de97703715768da9d872e30/based/models/mixers/linear_attention.py#L136)\n",
    "A_qk = torch.einsum(\"bhnd,bhmd->bhnm\", q, k) \n",
    "A_qk = torch.tril(A_qk)        \n",
    "y = torch.einsum(\"bhnm,bhme->bhne\", A_qk.to(q.dtype), v.to(q.dtype))\n",
    "z = 1 / (torch.einsum(\"bhld,bhld->bhl\", q, k.cumsum(2)) + eps)\n",
    "y = y * z[..., None]\n",
    "y = rearrange(y, 'b h l d -> b l (h d)')\n",
    "\n",
    "\n",
    "# Version 2: with decay\n",
    "use_decay_proj = True\n",
    "decay_proj = nn.Linear(d_model, h)\n",
    "cumsum_matrix = torch.tril(torch.ones((n, n))).to(q.device, q.dtype)\n",
    "\n",
    "decay = decay_cls()\n",
    "decay, decay_recurrent = decay if decay is not None else (None, None)\n",
    "\n",
    "A_qk = torch.einsum(\"bhnd,bhmd->bhnm\", q, k) \n",
    "if decay is not None:\n",
    "    decay = decay[:, :n, :n]\n",
    "    if len(decay.shape) == 3:\n",
    "        decay = decay.unsqueeze(0)\n",
    "    if use_decay_proj:\n",
    "        dt_out = decay_proj(hidden_states) # (b l d) --> (b, l, h)\n",
    "        assert decay.shape[2] >= n, f\"decay matrix {decay.shape} to short for sequence length {l}\"\n",
    "        decay_mat = dt_out.transpose(1,2).unsqueeze(-1) * decay   # (b, h, l, 1) * (1, h, l, l)\n",
    "    elif decay is not None:\n",
    "        decay_mat = decay\n",
    "    A_qk = A_qk * decay_mat\n",
    "else:\n",
    "    A_qk = A_qk * cumsum_matrix       \n",
    "out = torch.einsum(\"bhnm,bhme->bhne\", A_qk.to(hidden_states.dtype), v.to(hidden_states.dtype))\n",
    "z = 1 / (torch.einsum(\"bhld,bhld->bhl\", q, k.cumsum(2)) + eps)\n",
    "y = out * z[..., None]\n",
    "y = y.to(hidden_states.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "based",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
