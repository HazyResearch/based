{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run the setup.py in the based/ directory to install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set to download to a path with sufficient space\n",
    "! export TRANSFORMERS_CACHE=/var/cr05_data/sim_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/cr05_data/sim_data/miniconda3/envs/based/lib/python3.8/site-packages/tqdm-4.66.1-py3.8.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'causal_attention_cuda'\n",
      "No module named 'causal_attention_cuda'\n",
      "Successfully imported the causal dot product kernel! \n",
      "Successfully imported the FLA triton kernels! \n"
     ]
    }
   ],
   "source": [
    "# Step 3: Download the Based model\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from based.models.gpt import GPTLMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPTLMHeadModel.from_pretrained_hf(\"hazyresearch/based-360m\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: download the baselines\n",
    "do_download = False\n",
    "\n",
    "if do_download:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer\n",
    "    from based.models.mamba import MambaLMHeadModel\n",
    "    from based.models.transformer.gpt import GPTLMHeadModel as AttentionGPTLMHeadModel\n",
    "\n",
    "    # Attention\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    model = AttentionGPTLMHeadModel.from_pretrained_hf(\"hazyresearch/attn-360m\").to(\"cuda\")\n",
    "\n",
    "    # Mamba\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    model = MambaLMHeadModel.from_pretrained_hf(\"hazyresearch/mamba-360m\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLMHeadModel(\n",
       "  (transformer): GPTModel(\n",
       "    (embeddings): GPT2Embeddings(\n",
       "      (word_embeddings): Embedding(50264, 1024)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): Block(\n",
       "        (mixer): BaseConv(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (conv): ShortConvolution(\n",
       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (mixer): LinearAttention(\n",
       "          (feature_map): TaylorExp()\n",
       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (mixer): SlidingAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "          (inner_attn): FlashSelfAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): FlashCrossAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (3-5): 3 x Block(\n",
       "        (mixer): BaseConv(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (conv): ShortConvolution(\n",
       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (mixer): LinearAttention(\n",
       "          (feature_map): TaylorExp()\n",
       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (mixer): SlidingAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "          (inner_attn): FlashSelfAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): FlashCrossAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (8-10): 3 x Block(\n",
       "        (mixer): BaseConv(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (conv): ShortConvolution(\n",
       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (mixer): LinearAttention(\n",
       "          (feature_map): TaylorExp()\n",
       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (12): Block(\n",
       "        (mixer): SlidingAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "          (inner_attn): FlashSelfAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): FlashCrossAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (13-15): 3 x Block(\n",
       "        (mixer): BaseConv(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (conv): ShortConvolution(\n",
       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (16): Block(\n",
       "        (mixer): LinearAttention(\n",
       "          (feature_map): TaylorExp()\n",
       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (17): Block(\n",
       "        (mixer): SlidingAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "          (inner_attn): FlashSelfAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): FlashCrossAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (18-20): 3 x Block(\n",
       "        (mixer): BaseConv(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (conv): ShortConvolution(\n",
       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (21): Block(\n",
       "        (mixer): LinearAttention(\n",
       "          (feature_map): TaylorExp()\n",
       "          (proj_q): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_k): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (proj_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (22): Block(\n",
       "        (mixer): SlidingAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=False)\n",
       "          (inner_attn): FlashSelfAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): FlashCrossAttention(\n",
       "            (drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out_proj): FusedDense(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "      (23-26): 4 x Block(\n",
       "        (mixer): BaseConv(\n",
       "          (in_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (conv): ShortConvolution(\n",
       "            (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): RMSNorm()\n",
       "        (mlp): GatedMlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (fc2): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (drop_f): Dropout(p=0, inplace=False)\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Inspect the hybrid structure of Based\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample next token predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Stan -> ford\n",
      "ford ->  University\n",
      " university -> ,\n",
      " is ->  a\n",
      " in ->  the\n",
      " the ->  process\n",
      " state ->  of\n",
      " of ->  California\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = tokenizer(\"Stanford university is in the state of\", return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(**input)\n",
    "print(len(output.logits[0]))\n",
    "max = output.logits.argmax(dim=-1)[0]\n",
    "\n",
    "# next token predictions\n",
    "for tok, out_tok in zip(input[\"input_ids\"][0], max):\n",
    "    print(f\"{tokenizer.decode(tok.item())} -> {tokenizer.decode(out_tok.item())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation with Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start=19, limit=21\n",
      "The capital of California is Sacramento. The capital of Italy is Rome. The capital of France is ->  Paris.\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "input_text = \"The capital of California is Sacramento. The capital of Italy is Rome. The capital of France is\" \n",
    "\n",
    "context_length = 2048\n",
    "generation_length = 2\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "    [input_text], return_tensors=\"pt\", padding=True, truncation=True, max_length=context_length\n",
    ").input_ids.to(\"cuda\")\n",
    "\n",
    "limit = inputs.shape[-1] + generation_length\n",
    "start = inputs.shape[-1]\n",
    "print(f\"{start=}, {limit=}\")\n",
    "\n",
    "# Generate\n",
    "model.eval()\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        fn = model.generate\n",
    "        generations = fn(\n",
    "            input_ids=inputs,\n",
    "            max_length=limit,\n",
    "            temperature=0.1,\n",
    "            top_k=1,\n",
    "            top_p=1.0,\n",
    "        )\n",
    "        preds = generations[:, start:]\n",
    "        pred_ids =  preds[0].tolist()\n",
    "        pred = tokenizer.decode(pred_ids)\n",
    "        input_text = tokenizer.decode(inputs[0].tolist())  \n",
    "\n",
    "print(f\"{input_text} -> {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
