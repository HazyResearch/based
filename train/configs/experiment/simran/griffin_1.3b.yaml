# @package _global_
defaults:
  - /experiment/pile/gpt3m-flash.yaml
  - override /model: griffin

train: 
  optimizer:
    lr: 0.0008
    betas: [0.9, 0.95]
    _target_: apex.optimizers.FusedAdam
    adam_w_mode: true
    weight_decay: 0.1
  
  scheduler: 
    lr_min: 0.00008
    _target_: train.optim.timm_lr_scheduler.TimmCosineLRScheduler
    warmup_t: 200
    t_initial: 19800
    t_in_epochs: false
    warmup_prefix: true
    warmup_lr_init: 0.000001

expt_name: 04-15-griffin-width-1024
name: ${.expt_name}

callbacks:
  model_checkpoint:
    dirpath: /var/cr05_data/sim_data/checkpoints/${expt_name}


trainer: 
  # this interval is in terms of batch_idx not in terms of global_step, so we need 
  # to multiply by accumulate_grad_batches
  val_check_interval: ${eval:1000 * ${.accumulate_grad_batches}}
  max_steps: 20000

datamodule:
  _target_: train.datamodules.language_modeling_neox.NeoxLMDataModule   
  batch_size: 8  # per gpu
  batch_size_eval: 8
  num_predict_batches_eval: 100
  global_batch_size: ${..train.global_batch_size}
  max_steps: ${..trainer.max_steps}
  num_test_samples: 1000
  num_valid_samples: 1000

model:
  config:
    vocab_size: 50277
    width: 1024
    mlp_expanded_width: 3072
    num_heads: 16
    num_layers: 26 
    lru_width: 1024
    embeddings_scale_by_sqrt_dim: True
    attention_window_size: 1024
    logits_soft_cap:  30.0

